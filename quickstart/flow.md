### 创建数据源
在工作流编辑器中，我们首先看到的节点是数据源，这个节点用来接收用户的实时数据，也就是说，当这个节点被创建后，用户需要将自己的数据推送至这个数据源中，才可以继续进行下一步。

数据源节点包含两部分内容： 

1.名称：用来标识这个节点的唯一性，同时这个名字也作为工作流的名称。

2.字段信息：用来定义数据结构；在类型方面，目前支持：

```
string
float
long
date
boolean
array[string/long/float]
```

共计八种类型。

!> 注意：`float`类型的精度是64位。

!> 注意：每一个数据源/消息队列中的每一条数据都将被保存2天时间，超过时间后自动删除。

> 数据源其实也是一个消息队列，技术人员可以将它理解成kafka中的一个topic。

**数据源节点填写参数说明：**

* 名称：必填项；命名规则: 1-128个字符,支持小写字母、数字、下划线；必须以大小写字母或下划线开头。
* 字段名称：必填项；用来标识字段唯一性；命名规则: 1-128个字符,支持小写字母、数字、下划线；必须以大小写字母或下划线开头。
* 字段类型：必填项；描述字段的数据类型。

填写完成相应信息后，点击**创建**按钮，第一个工作流就创建成功了。

**操作演示：**

![](_media/flow1.gif)

### 计算任务

我们可以对数据源节点中的数据进行计算，计算完成后，会生成一个新的消息队列来存放计算结果，我们也可以对消息队列节点中的数据进行计算，这样就形成了一个复杂的工作流程。

计算的方式分为两种：标准SQL计算和自定义代码计算，它们两者可以并存，执行的顺序是先执行自定义代码计算，后执行标准SQL计算；在一个计算任务中，至少需要指定一种计算方式。

SQL计算就是自己编写SQL语句，对数据源或消息队列中的数据进行分批处理，并且以一定的时间粒度(可以自行设置，最小的粒度为5秒)进行聚合，将结果自动发送到一个新的消息队列中。

计算任务是消耗物理资源的，所以每一个计算任务节点都需要为之分配**计算资源**（CPU & 内存）， 不同的计算任务之间的计算资源互相隔离，互不影响。

!> 注意：SQL中，from关键字之后的table名称为**stream**

>高级用法：自定义计算请见下文

**操作演示：**

![](_media/flow2.gif)

### 自定义计算（Plugin）- Java

#### 下载&加载项目

**操作流程：**

首先，在下方中，下载Plugin-Java项目工程。

点击右侧链接下载 -> [Plugin-Java.zip](http://7xtfy5.com1.z0.glb.clouddn.com/Pandora-Plugin-Java.zip)

解压后，使用Java IDE导入Pandora-Plugin项目。

等待项目依赖加载完成后，可以在 `src/main/java/demo/` 目录下查看一个简单的示例。

而 `src/main/java/userWrite/` 目录下是用户自行编写代码的地方。

**操作演示：**

![](_media/flow3.gif)

#### 编写输入&输出类

**操作流程：**

首先，我们要定义我们编写代码的输入和输出。

输入通常是工作流中的消息队列，那么我们只需要将消息队列中的字段信息原封不动的复制一份即可，而输出是由我们自己编写的代码决定的，所以也需要提前定义好我们的输出信息。

负责数据源的类是 `src/main/java/userWrite/bean/SourceData.java`

负责输出数据的类是 `src/main/java/userWrite/bean/OutputData.java`

我们只需要根据 `src/main/java/demo/` 目录下的示例进行编写即可，代码中也有很详细的注释。

#### 编写业务逻辑代码

业务逻辑代码的编写在 `src/main/java/userWrite/customLogic/customlogic.java` 中的**parse**方法中。

!>注意：customlogic.java 此类名称可自由更改；最终的返回必须是List&lt;OutputData&gt;。

#### 打包上传

代码编写完成后，需要打成Jar包，然后上传至Pandora平台中。

!>注意：Jar包名称必须和 `src/main/java/userWrite/customLogic/` 目录下包含parse方法的类名称一致。

**操作演示：**

![](_media/flow4.gif)


### 数据导出

任何一个数据源和消息队列节点中的数据，都可以导出。

目前我们支持将数据导出到对象存储、时序数据库、日志检索服务、HTTP服务器，并且支持导出到RDS，如果您有这方面的需求，请通过工单或客服联系我们。

### 导出数据至HTTP

**导出任务节点填写参数说明：**

* 名称：必填项；用来标识这个导出任务的唯一性；命名规则: 1-128个字符,支持小写字母、数字、下划线；必须以大小写字母或下划线开头。
* 服务器地址：必填项；服务器地址（ip或域名），例如:`https://pipeline.qiniu.com` 或 `127.0.0.1:7758`。
* 请求资源路径：必填项；请求资源路径（具体地址,不包含ip或域名），例如:`/test/repos`。

!> 注意: 导出数据格式和`推送数据`相同。

**操作演示：**

![](_media/flow5.gif)

### 导出数据至对象存储

**导出任务节点填写参数说明：**

* 名称：必填项；用来标识这个导出任务的唯一性；命名规则: 1-128个字符,支持小写字母、数字、下划线；必须以大小写字母或下划线开头。
* 空间名称：必填项；对象存储中的Bucket名称。
* 文件前缀：非必填项；导出的文件名称的前缀。
* 导出账号：必填项；空间名称所属用户的七牛账户名称。（目前允许跨账号导出）
* 账号公钥：必填项；七牛账户的公钥，个人面板-密钥管理中查看`AK`即公钥。
* 导出类型：必填项；可以将文件导成三种格式：json、text、parquet；其中json和text可以选择是否将文件压缩，而parquet无需选择，默认自动压缩，压缩比大概为3-20倍。
* 最大文件保存天数：必填项；文件存储时效，超过这个时间范围的文件会被自动删除，为0的话则永久存储。

**操作演示：**

![](_media/flow6.gif)

### 导出数据至日志检索服务

**导出任务节点填写参数说明：**

* 名称：必填项；用来标识这个导出任务的唯一性；命名规则: 1-128个字符,支持小写字母、数字、下划线；必须以大小写字母或下划线开头。
* 数据仓库：选择或输入一个数据仓库，数据将会被导入到这个仓库当中。
* 字段信息：需要将消息队列和日志仓库中的字段信息一一对应。

!>注意：如果您还没有创建日志仓库，那么请参见[日志检索服务](/quickstart/logdb)

> 消息队列中,字段的类型与日志检索服务中的字段类型需要作出如下对应:
> 
> 消息队列类型:string 对应 日志检索服务:string / date
> 
> 消息队列类型:long 对应 日志检索服务:long / date
> 
> 消息队列类型:float 对应 日志检索服务:float
> 
> 消息队列类型:date 对应 日志检索服务:date 
> 
> 消息队列类型:boolean 对应 日志检索服务:boolean
> 
> 消息队列类型:array[string] 对应 日志检索服务:string
> 
> 消息队列类型:array[long] 对应 日志检索服务:long
> 
> 消息队列类型:array[float] 对应 日志检索服务:float

**操作演示：**

![](_media/flow7.gif)

### 导出数据至时序数据库

**导出任务节点填写参数说明：**

* 名称：必填项；用来标识这个导出任务的唯一性；命名规则: 1-128个字符,支持小写字母、数字、下划线；必须以大小写字母或下划线开头。
* 数据仓库：必填项；选择或输入一个数据仓库。
* 序列：必填项；选择或输入一个序列，数据将会被导入到这个序列当中。
* 时间戳：非必填项；指定一个时间，会用rfc3339日期格式进行解析,如果格式不正确则会抛弃这一条数据,如果此项为空,则默认使用当前时间。
* 字段信息：

!>注意：如果您还没有创建数据仓库和序列或不了解其具体含义，那么请参见[时序数据库](/quickstart/tsdb)

**操作演示：**

![](_media/flow8.gif)

### 复杂的业务逻辑

工作流编辑器可以完成非常复杂的数据业务，它的整个架构是完全`拓扑逻辑`的。

依靠`消息队列`作为数据中间件，数据的计算可以往复循环，以达到满足复杂业务的需求。

**操作演示：**

![](_media/flow9.gif)

### 便捷操作

`复制&粘贴`动作，快速完成重复工作。

`复制&粘贴`动作可以仅复制数据结构，也可以复制数据结构+内容。

**操作演示：**

![](_media/flow10.gif)